{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detection and Location Chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**: This hackathon project represents our effort to combine our existing machine learning and photogrametry efforts and further combine those efforts with both Cloud and Edge based solutions based upon Xilinx FPGA acceleration. \n",
    "\n",
    "The Trimble team decided that the Xilinx hackathon would provide an excellent oppertunity to take the first steps in combining these technologies and learning how to use the varied Xilinx techologies.\n",
    "\n",
    "Our initial hope was to use a TensorFlow system to provide the machine learning component of our test based on an AWS Ultrascale instance. That technology was unavailable for the hackathon, so during the event we trained a system based on a more stardard AWS Tensorflow instance and accessed that instance via Pynq networking.\n",
    "\n",
    "The Team Trimble is composed of\n",
    "\n",
    "* Roy Godzdanker – Trimble Product Architect for ICT\n",
    "* Robert Banefield – Trimble Data Machine Learning Specialist\n",
    "* Vinod Khare – Trimble ICT Photogrammetry\n",
    "* Ashish Khare – Trimble Geomatics Photogrammetry\n",
    "* Young-Jin Lee – Trimble ICT Photogrammetry\n",
    "* Matt Compton - Trimble ICT Design Engineer\n",
    "\n",
    "_NOTES_:\n",
    "\n",
    "1. The TensorFlow system is sitting at an AWS instance. This is the slow and simple one for my debug effort. In the spirit of the hackathon, we started in training at the beginning of the night. This implies that it's capabilities were not exceptional at the beginning of the night and it will be better as the newly trained net is swapped in in the morning. Further tests back at the ranch will include testing this chain against some of the other theoretical models. The current net underperforms some previous efforts, further exploration is needed here\n",
    "\n",
    "2. We also need to explore the TensorFlow element as an edge device. Advances in Xilinx FPGA tools may make that cost competative with a GPU box.\n",
    "\n",
    "3. Xilinx HLS looks to be able to add needed acceleration functions but this needs further exploration going forward. We explored the idea of overly with python controled DMA, this is very promising\n",
    "\n",
    "The following are globals used within this project To Change this to different image set, simply change the images indicated and run through the notebook again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Camera data is sent to the system from a remote repository. \n",
    "2. The Camera Data is sent to the Pynq to being processing.\n",
    "3. The TensorFlow cloud delivers metadata for the images that were transferred to it back to the Pynq via net transfer\n",
    "4. The Pynq software uses the photogrammetric OpenCV software chain that we wrote to estimate and calculate geometric position. In addition, images are displayed on the HDMI monitor and LCD display so we can see what is going on and to serve as a debug aid\n",
    "5. The calculated position of the object is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy\n",
    "import matplotlib.patches  as patches\n",
    "import pynq.overlays.base\n",
    "import pynq.lib.arduino as arduino\n",
    "import pynq.lib.video as video\n",
    "import requests\n",
    "import scipy\n",
    "import sys\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Config\n",
    "gAWS_TENSORFLOW_INSTANCE = 'http://34.202.159.80'\n",
    "gCAMERA0_IMAGE = \"/home/xilinx/jupyter_notebooks/trimble-mp/CAM2_image_0032.jpg\"\n",
    "gCAMERA1_IMAGE = \"/home/xilinx/jupyter_notebooks/trimble-mp/CAM3_image_0032.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn on the HDMI coming off the pink board. This is used in a fashion that is different than their primary test notes and may be difficult to complete during the time period. Specifically, the hdmi out is used without the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = pynq.overlays.base.BaseOverlay(\"base.bit\")\n",
    "hdmi_in = base.video.hdmi_in\n",
    "hdmi_out = base.video.hdmi_out\n",
    "v = video.VideoMode(1920,1080,24)\n",
    "hdmi_out.configure(v, video.PIXEL_BGR)\n",
    "hdmi_out.start()\n",
    "outframe = hdmi_out.newframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pillow, pull in the chosen image for Camera 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read images\n",
    "image0BGR = cv2.imread(gCAMERA0_IMAGE)\n",
    "image1BGR = cv2.imread(gCAMERA1_IMAGE)\n",
    "\n",
    "image0 = image0BGR[...,::-1]\n",
    "image1 = image1BGR[...,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do exactly the same for the second image of the overlapping pair from camera 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send one of these to the HDMI, we are going to have to reformat it to fit the provided HDMI display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show image 0 on HDMI\n",
    "\n",
    "# Need to resize it first\n",
    "outframe[:] = cv2.resize(image0BGR, (1920, 1080));\n",
    "hdmi_out.writeframe(outframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also display Young-Jin to the LCD screen. Why ? Because Young Jin does awesome work and deserves to be famous and also because I can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Show image on LCD\n",
    "\n",
    "# Open LCD object and clear\n",
    "lcd = arduino.Arduino_LCD18(base.ARDUINO)\n",
    "lcd.clear()\n",
    "\n",
    "# Write image to disk\n",
    "nw = 160\n",
    "nl = 128\n",
    "cv2.imwrite(\"/home/xilinx/small.jpg\", cv2.resize(image0BGR, (nw,nl)))\n",
    "\n",
    "# Display!\n",
    "lcd.display(\"/home/xilinx/small.jpg\",x_pos=0,y_pos=127,orientation=3,background=[255,255,255])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to classify the images. This runs the remote version of TensorFlow on the image to get the bounding box. The following routine wraps this for simplicity. The spun up AWS TensorFlow instance is expecting to get be\n",
    "sent a JPEG and will classify and send back the results as JSON.\n",
    "\n",
    "The IP address of the spun up AWS instance is given by the global gAWS_TENSORFLOW_INSTANCE which is specified at the\n",
    "beginning of this note book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RemoteTensorFlowClassify(image_name_string):\n",
    "    f = open(image_name_string,'rb')\n",
    "    r = requests.put(gAWS_TENSORFLOW_INSTANCE, data=f)\n",
    "    return json.loads(r.content.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually call the defined function on images from camera 1 and camera 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_id': 1,\n",
       " 'score': 0.9912543892860413,\n",
       " 'xmax': 1125.3021841049194,\n",
       " 'xmin': 949.6559209823608,\n",
       " 'ymax': 2049.9720282554626,\n",
       " 'ymin': 1536.3477578163147}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return the object that camera zero sees with the maximum score\n",
    "cam0_json_return = RemoteTensorFlowClassify(gCAMERA0_IMAGE)\n",
    "json0 = cam0_json_return[\"image_detection\"]\n",
    "max = 0.0\n",
    "out = []\n",
    "for var in json0['object']:\n",
    "    if (var['score'] > max):\n",
    "        out = var\n",
    "json0 = out\n",
    "json0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_id': 1,\n",
       " 'score': 0.9984133243560791,\n",
       " 'xmax': 929.8188982009888,\n",
       " 'xmin': 559.6235628128052,\n",
       " 'ymax': 2048.9980235099792,\n",
       " 'ymin': 1594.049328327179}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Return the object that camera one sees with the maximum score\n",
    "cam1_json_return = RemoteTensorFlowClassify(gCAMERA1_IMAGE)\n",
    "json1 = cam1_json_return[\"image_detection\"]\n",
    "max = 0.0\n",
    "out = []\n",
    "for var in json1['object']:\n",
    "    if (var['score'] > max):\n",
    "        out = var\n",
    "json1 = out\n",
    "json1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AWS tensorflow reports the bounding boxes for the required object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DrawRect(the_json,the_image, x1, x2, y1, y2 ): \n",
    "    # Currently offline until the TesnorFlow net is fixed\n",
    "    #x1 = int(the_json[\"xmin\"]) \n",
    "    #y1 = int(the_json[\"ymin\"]) \n",
    "    #x2 = int(the_json[\"xmax\"]) \n",
    "    #y2 = int(the_json[\"ymax\"])\n",
    " \n",
    "    \n",
    "    fig, ax = pyplot.subplots(1)\n",
    "    ax.imshow(the_image)\n",
    "    rect = patches.Rectangle((x1,y1), (x2-x1), (y2-y1), linewidth = 1 , edgecolor = 'r', facecolor='none') \n",
    "    ax.add_patch(rect)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert to grayscale\n",
    "grayImage0 = cv2.cvtColor(image0, cv2.COLOR_RGB2GRAY)\n",
    "grayImage1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IsInsideROI(pt, the_json, x1, x2, y1, y2):\n",
    "#    x_min = int(the_json[\"object\"][\"xmin\"])\n",
    "#    y_min = int(the_json[\"object\"][\"ymin\"])\n",
    "#    x_max = int(the_json[\"object\"][\"xmax\"])\n",
    "#    y_max = int(the_json[\"object\"][\"ymax\"])\n",
    " \n",
    "    x_min = x1\n",
    "    y_min = y1\n",
    "    x_max = x2\n",
    "    y_max = y2\n",
    "    if(pt[0]>=x_min and pt[0] <=x_max and pt[1]>=y_min and pt[1]<=y_max):\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Detect keypoints\n",
    "Brisk = cv2.BRISK_create()\n",
    "\n",
    "keyPoints0 = Brisk.detect(grayImage0)\n",
    "keyPoints1 = Brisk.detect(grayImage1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Find keypoints inside ROI\n",
    "roiKeyPoints0 = numpy.asarray([k for k in keyPoints0 if IsInsideROI(k.pt,json0, 955, 1045, 740, 1275 )])\n",
    "roiKeyPoints1 = numpy.asarray([k for k in keyPoints1 if IsInsideROI(k.pt,json1, 1335, 1465, 910, 1455 )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Compute descriptors for keypoitns inside ROI\n",
    "[keyPoints0, desc0] = Brisk.compute(grayImage0, roiKeyPoints0);\n",
    "[keyPoints1, desc1] = Brisk.compute(grayImage1, roiKeyPoints1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Find matches of ROI keypoints\n",
    "BF = cv2.BFMatcher()\n",
    "matches = BF.match(desc0, desc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract pixel coordinates from matched keypoints\n",
    "\n",
    "x_C0 = numpy.asarray([keyPoints0[match.queryIdx].pt for match in matches])\n",
    "x_C1 = numpy.asarray([keyPoints1[match.trainIdx].pt for match in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full mesh triangularization is off line until we reconsile the camera calibration. There was an issue discovered during the hackathon that needs to be examined in teh lab setup s the code below this will not function until we reconsile the camera calibration config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (3,4) not aligned: 5 (dim 0) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e8dd714cfa68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mP_C0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_C0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_C0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_C0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mP_C1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_C1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_C1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_C1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,) and (3,4) not aligned: 5 (dim 0) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Triangulate points\n",
    "\n",
    "# We need projection matrices for camera 0 and camera 1\n",
    "f = 8.350589e+000 / 3.45E-3\n",
    "cx = -3.922872e-002 / 3.45E-3\n",
    "cy = -1.396717e-004 / 3.45E-3\n",
    "K_C0 = numpy.transpose(numpy.asarray([[f, 0, 0], [0, f, 0], [cx, cy, 1]]))\n",
    "k_C0 = numpy.asarray([1.761471e-003, -2.920431e-005, -8.341438e-005, -9.470247e-006, -1.140118e-007])\n",
    "\n",
    "[R_C0, J] = cv2.Rodrigues(numpy.asarray([1.5315866633, 2.6655790203, -0.0270418317]))\n",
    "T_C0 = numpy.transpose(numpy.asarray([[152.9307390952, 260.3066944976, 351.7405264829]])) * 1000\n",
    "\n",
    "f = 8.259861e+000 / 3.45E-3\n",
    "cx = 8.397453e-002 / 3.45E-3\n",
    "cy = -2.382030e-002 / 3.45E-3\n",
    "K_C0 = numpy.transpose(numpy.asarray([[f, 0, 0], [0, f, 0], [cx, cy, 1]]))\n",
    "K_C1 = numpy.asarray([1.660053e-003, -2.986269e-005, -7.461966e-008, -2.247960e-004, -2.290483e-006])\n",
    "\n",
    "[R_C1, J] = cv2.Rodrigues(numpy.asarray([1.4200199799, -2.6113619450, -0.1371719827]))\n",
    "T_C1 = numpy.transpose(numpy.asarray([[146.8718203137, 259.9661037150, 351.5832136366]])) * 1000\n",
    "\n",
    "P_C0 = numpy.dot(K_C0,numpy.concatenate((R_C0, T_C0), 1))\n",
    "P_C1 = numpy.dot(K_C1,numpy.concatenate((R_C1, T_C1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P_C1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d8238d5793f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute 3D coordinates of detected points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_C0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvertPointsFromHomogeneous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriangulatePoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_C0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_C1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_C0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_C1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'P_C1' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute 3D coordinates of detected points\n",
    "X_C0 = cv2.convertPointsFromHomogeneous(numpy.transpose(cv2.triangulatePoints(P_C0, P_C1, numpy.transpose(x_C0), numpy.transpose(x_C1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
